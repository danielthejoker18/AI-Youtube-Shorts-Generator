# LLM Provider Configuration
# Choose provider: 'openai', 'groq', or 'ollama'
LLM_PROVIDER=openai  # openai, groq, ollama
OPENAI_API_KEY=your-key-here
GROQ_API_KEY=your-key-here
OLLAMA_HOST=http://localhost:11434

# API Keys
OPENAI_API=your_openai_api_key_here
GROQ_API_KEY=your_groq_api_key_here

# Ollama Configuration (if using local Ollama)
OLLAMA_MODEL=mixtral  # or any other model you have pulled locally
